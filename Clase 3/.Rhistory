return_list <- list("none",
"none")
names(return_list) <- plot_type
return_list[[max_index]] <- "bottom"
return(return_list)
}
legend_list <- PickLegend(list(nurse_party_props, resi_party_props))
nurse_bed_third_party <- ggplot(nurse_party_props, aes(x=collection_year_n)) +
geom_line(aes(y=value, color=name, group=name), size = 0.8) +
geom_point(aes(y=value, color=name), size = 3) +
theme_stata() +
theme(legend.title = element_blank(),
axis.text.y = element_text(angle = 0)) +
scale_x_continuous(breaks = year) +
labs(title = "Nursing Homes", x = "", y = "Index relative to 2015")
resi_bed_third_party <- ggplot(resi_party_props, aes(x=collection_year_n)) +
geom_line(aes(y=value, color=name,group=name), size = 0.8) +
geom_point(aes(y=value, color=name), size = 3) +
theme_stata() +
theme(legend.title = element_blank(),
axis.text.y = element_text(angle = 0)) +
scale_x_continuous(breaks = year) +
labs(title = "Residential Homes", x = "", y = "Index relative to 2015")
plot_list <- list(nurse_bed_third_party, resi_bed_third_party)
range_to_use <- GetAxisRangeMax(plot_list)
top <-  nurse_bed_third_party +
theme(plot.margin = margin(0.035, 0.035, 0, 0.035, unit = "npc")) +
scale_y_continuous(limits = range_to_use)
bottom <- resi_bed_third_party +
theme(plot.margin = margin(0.035, 0, 0.035, 0.035, unit = "npc")) +
scale_y_continuous(limits = range_to_use)
combined <- (top / bottom & theme(legend.position = "bottom"))   +
plot_layout(guides = "collect")
return(combined)
}
lapply("ALL LD",
viii_beds_over_time_function)
viii_beds_over_time_function <- function(split_type = "All LD"){
stopifnot(split_type %in% names(data_list))
data_to_use <- data_list[[split_type]]
short_name <- short_names[[split_type]]
su_beds <- data_to_use %>% filter(is.na(service_type_care_home_service_with_nursing)==FALSE) %>%
select(collection_year_n, cqc_location_id,
service_type_care_home_service_with_nursing, care_homes_beds, commissioning_la) %>%
distinct() %>%
group_by(collection_year_n, service_type_care_home_service_with_nursing, commissioning_la) %>%
summarise(sum_beds = sum(care_homes_beds))  %>%
pivot_wider(id_cols = c("collection_year_n", "commissioning_la"),
names_from = "service_type_care_home_service_with_nursing",
values_from = "sum_beds") %>%
mutate(across(c("Nursing homes" , "Residential homes"),
~ fifelse(is.na(.x), 0, .x)))
su_beds_clean <- FixSummarisedData(su_beds, c("Nursing homes" , "Residential homes"))
# stopifnot(!any(unlist(lapply(su_beds_clean, is.infinite))))
# stopifnot(!any(is.na(su_beds_clean)))
ics_su <- su_beds_clean %>%
group_by(stp, collection_year_n) %>%
summarise(across(c("Nursing homes" , "Residential homes"),
~ mean(.x)))
london <- su_beds_clean %>%
group_by(collection_year_n) %>%
summarise(across(c("Nursing homes" , "Residential homes"),
~ mean(.x)))
su_binded <- rbindlist(list(su_beds_clean,
ics_su,
london),
fill = T) %>%
mutate(area_type = fcase(!is.na(commissioning_la) & !is.na(stp), commissioning_la,
is.na(commissioning_la) & !is.na(stp), stp,
is.na(commissioning_la) & is.na(stp), "London"))
# This is very slow. Is it worth speeding up? Probably not.
# Calculations could come first and then we just need to filter
BoroughFunction <- function(borough = "Sutton"){
short_ics_for_legend <- stp_dt[commissioning_la == borough, short_stpname]
CoL <- data.frame('City Of London', 'East London', 'NEL')
names(CoL) <- c('commissioning_la', 'StpName', 'short_stpname')
stp_dt <- rbind(stp_dt, CoL)
ics_to_use <- stp_dt[commissioning_la == borough, StpName]
plot_data <- su_binded %>%
filter(area_type %in% c(borough, ics_to_use, "London"))
short_ics_for_legend <- stp_dt[commissioning_la == borough, short_stpname]
rename_vector <- c(borough, ics_to_use, "London")
names(rename_vector) <- c("Borough", "ICS", "London")
new_merged_beds <- plot_data %>%
select(-c(commissioning_la, stp)) %>%
pivot_longer(c("Nursing homes" , "Residential homes")) %>%
pivot_wider(names_from = "area_type", values_from = "value") %>%
rename("service_type_care_home_service_with_nursing" = "name") %>%
rename(rename_vector) %>%
group_by(collection_year_n)
factor_levels <- c(paste0(borough, " trend"),
paste0(short_ics_for_legend, " trend"),
"London trend")
# nurse_beds <- new_merged_beds %>%
#   filter(service_type_care_home_service_with_nursing == "Nursing homes")
#
# nurse_beds <- nurse_beds %>%
#   cbind(nurse_beds$ICS[1], nurse_beds$London[1], nurse_beds$Borough[1]) %>%
#   mutate(ics_index = round(ICS/nurse_beds$ICS[1], digits=2), #ICS trend wrt to 2015 ICS value
#          ldn_index = round(London/nurse_beds$London[1], digits=2), #LDN trend wrt to 2015 LDN value
#          Borough = ifelse(is.na(Borough)==TRUE,ics_index*nurse_beds$Borough[1],Borough),
#          "ICS trend" = ics_index*(nurse_beds$Borough[1]),
#          "London trend" = ldn_index*(nurse_beds$Borough[1]))
#
# resi_beds <- new_merged_beds %>%
#   filter(service_type_care_home_service_with_nursing == "Residential homes")
#
# resi_beds <- resi_beds %>%
#   cbind(resi_beds$ICS[1], resi_beds$London[1], resi_beds$Borough[1]) %>%
#   mutate(ics_index = round(ICS/resi_beds$ICS[1], digits=2), #ICS trend wrt to 2015 ICS value
#          ldn_index = round(London/resi_beds$London[1], digits=2), #LDN trend wrt to 2015 LDN value
#          Borough = ifelse(is.na(Borough)==TRUE,ics_index*resi_beds$Borough[1],Borough),
#          "ICS trend" = ics_index*(resi_beds$Borough[1]),
#          "London trend" = ldn_index*(resi_beds$Borough[1]))
nurse_beds <- new_merged_beds %>% filter(service_type_care_home_service_with_nursing =="Nursing homes") %>% arrange(collection_year_n)
nurse_beds <- nurse_beds %>%
#cbind(nurse_beds$`ICS`[1],nurse_beds$`London`[1], nurse_beds$`Borough`[1]) %>%
mutate(
"ICS trend" = round(`ICS`/ nurse_beds$`ICS`[1], digits=2)*100, #ICS trend wrt to 2015 ICS value
"London trend" = round(`London`/ nurse_beds$`London`[1], digits=2)*100, #LDN trend wrt to 2015 LDN value
"Borough trend" = round(`Borough`/ nurse_beds$`Borough`[1], digits = 2)*100,
)
resi_beds <- new_merged_beds %>% filter(service_type_care_home_service_with_nursing =="Residential homes") %>% arrange(collection_year_n)
resi_beds <- resi_beds %>%
#cbind(resi_beds$`ICS`[1],resi_beds$`London`[1], resi_beds$`Borough`[1]) %>%
mutate(
"ICS trend" = round(`ICS`/ resi_beds$`ICS`[1], digits=2)*100, #ICS trend wrt to 2015 ICS value
"London trend" = round(`London`/ resi_beds$`London`[1], digits=2)*100, #LDN trend wrt to 2015 LDN value
"Borough trend" = round(`Borough`/ resi_beds$`Borough`[1], digits = 2)*100,
)
year <- c(2015,2016,2017,2018,2019,2020,2021)
##nursing beds
merged_beds_2 <- nurse_beds %>%
select(collection_year_n, "Borough trend", "ICS trend", "London trend")
merged_beds_3 <- merged_beds_2 %>%
pivot_longer(cols = c("Borough trend","ICS trend", "London trend")) %>%
arrange(name) %>%
mutate(name = gsub("ICS", short_ics_for_legend, name)) %>%
mutate(name = gsub("Borough", borough, name)) %>%
mutate(name = factor(name, factor_levels))
beds_nurse_longitudinal <- ggplot(merged_beds_3, aes(x=collection_year_n)) +
geom_line(aes(y=value, color=name,group=name), size = 0.8) +
geom_point(aes(y=value, color=name), size = 3) +
geom_line(aes(y=value ,color=name, group=name), size = 0.8) +
geom_point(aes(y=value,color=name), size = 3) +
theme_stata() +
theme(legend.title = element_blank(),
axis.text.y = element_text(angle = 0)) +
scale_x_continuous(breaks = year) +
labs(title = "Nursing homes", x = "", y = "Index relative to 2015")
##residential beds
merged_beds_2 <- resi_beds %>%
select(collection_year_n, "Borough trend", "ICS trend", "London trend")
merged_beds_3 <- merged_beds_2 %>%
pivot_longer(cols = c("Borough trend","ICS trend", "London trend")) %>%
arrange(name)  %>%
mutate(name = gsub("ICS", short_ics_for_legend, name)) %>%
mutate(name = gsub("Borough", borough, name))  %>%
mutate(name = factor(name, factor_levels))
beds_resi_longitudinal <- ggplot(merged_beds_3, aes(x=collection_year_n)) +
geom_line(aes(y=value, color=name,group=name), size = 0.8) +
geom_point(aes(y=value, color=name), size = 3) +
geom_line(aes(y=value ,color=name, group=name), size = 0.8) +
geom_point(aes(y=value,color=name), size = 3) +
theme_stata() +
theme(legend.title = element_blank(),
axis.text.y = element_text(angle = 0)) +
scale_x_continuous(breaks = year) +
labs(title = "Residential homes",
x = "", y = "Index relative to 2015") +
theme(legend.position = "none")
title <- paste0(split_type, " beds")
plot_list <- list(beds_nurse_longitudinal, beds_resi_longitudinal)
range_to_use <- GetAxisRangeMax(
plot_list = plot_list,
y_x_axis =  "y")
beds_nurse_longitudinal_2 <- beds_nurse_longitudinal +
theme(plot.margin = margin(0.035, 0.035, 0, 0.035, "npc")) +
scale_y_continuous(limits = range_to_use,
breaks = c(100, labeling::extended(range_to_use[[1]], range_to_use[[2]], 5)))
beds_resi_longitudinal_2 <- beds_resi_longitudinal +
theme(plot.margin = margin(0, 0.035, 0.035, 0.035, "npc")) +
scale_y_continuous(limits = range_to_use,
breaks = c(100, labeling::extended(range_to_use[[1]], range_to_use[[2]], 5)))
combined <- (beds_nurse_longitudinal_2 / beds_resi_longitudinal_2 & theme(legend.position = "bottom")) +
plot_layout(guides = "collect") +
plot_annotation(title = title)
save_name <- paste0(borough, "/", short_name, "_beds_nurse_res_longitudinal.rds")
rdsplot_SaveFunction(combined,
save_name,
level = "borough")
return(combined)
}
BoroughFunction('City Of London')
#lapply(stp_dt$commissioning_la, BoroughFunction)
}
lapply(names(data_list),
viii_beds_over_time_function)
viii_beds_over_time_function <- function(split_type = "All LD"){
}
lapply(names(data_list),viii_beds_over_time_function)
viii_beds_over_time_function <- function(split_type = "All LD"){
viii_beds_over_time_function <- function(split_type = "All LD"){
print(split_type)
stopifnot(split_type %in% names(data_list))
data_to_use <- data_list[[split_type]]
short_name <- short_names[[split_type]]
ldn_beds <- data_to_use %>%
filter(is.na(service_type_care_home_service_with_nursing)==FALSE & collection_year_n >= 2015) %>%
distinct() %>%
group_by(collection_year_n, service_type_care_home_service_with_nursing) %>%
summarise(total_beds=n(),
total_beds_topup = sum(care_homes_beds),
London = round((total_beds_topup/total_beds)*100, digits = 1) %>%
select(collection_year_n,service_type_care_home_service_with_nursing, value ='London') %>%
data.frame() |>
mutate(name = 'London')
ics_beds <- data_to_use %>%
viii_beds_over_time_function <- function(split_type = "All LD"){
stopifnot(split_type %in% names(data_list))
data_to_use <- data_list[[split_type]]
short_name <- short_names[[split_type]]
su_beds <- data_to_use %>% filter(is.na(service_type_care_home_service_with_nursing)==FALSE) %>%
select(collection_year_n, cqc_location_id,
service_type_care_home_service_with_nursing, care_homes_beds, commissioning_la) %>%
distinct() %>%
group_by(collection_year_n, service_type_care_home_service_with_nursing, commissioning_la) %>%
summarise(sum_beds = sum(care_homes_beds))  %>%
pivot_wider(id_cols = c("collection_year_n", "commissioning_la"),
names_from = "service_type_care_home_service_with_nursing",
values_from = "sum_beds") %>%
mutate(across(c("Nursing homes" , "Residential homes"),
~ fifelse(is.na(.x), 0, .x)))
su_beds_clean <- FixSummarisedData(su_beds, c("Nursing homes" , "Residential homes"))
# stopifnot(!any(unlist(lapply(su_beds_clean, is.infinite))))
# stopifnot(!any(is.na(su_beds_clean)))
ics_su <- su_beds_clean %>%
group_by(stp, collection_year_n) %>%
summarise(across(c("Nursing homes" , "Residential homes"),
~ mean(.x)))
london <- su_beds_clean %>%
group_by(collection_year_n) %>%
summarise(across(c("Nursing homes" , "Residential homes"),
~ mean(.x)))
su_binded <- rbindlist(list(su_beds_clean,
ics_su,
london),
fill = T) %>%
mutate(area_type = fcase(!is.na(commissioning_la) & !is.na(stp), commissioning_la,
is.na(commissioning_la) & !is.na(stp), stp,
is.na(commissioning_la) & is.na(stp), "London"))
return(su_binded)
}
lapply(names(data_list),
viii_beds_over_time_function)
library(ergm)
data("faux.dixon.high")
detach(package:ergm)
detach(package:network)
require(intergraph) ## This package lets you port network objects between igraph and the statnet suite of packages (of which ergm is a part). It's not perfect, but it's very useful.
require(igraph)
dixon<-asIgraph(faux.dixon.high)
summary(dixon)
plot(dixon,
vertex.label=NA,
edge.arrow.size=0.25,
vertex.size=3,
edge.width=0.5,
vertex.color=V(dixon)$grade
)
legend("topright",
legend=7:12,
pch=19,
col=categorical_pal(8)[c(7,8,1,2,3,4)]
) # A legend to help make sense of the colouring; take my word for it that these are the right colours for each grade!
plot(dixon,
vertex.label=NA,
edge.arrow.size=0.25,
vertex.size=3,
edge.width=0.5,
vertex.color=V(dixon)$grade
)
legend("topright",
legend=7:12,
pch=19,
col=categorical_pal(8)[c(7,8,1,2,3,4)]
) # A legend to help make sense of the colouring; take my word for it that these are the right colours for each grade!
dyad.census(dixon)
reciprocity(dixon)
## Using the dyad census
2*dyad_census(dixon)$mut/ecount(dixon) ## we're doing 2 times the mutual count to get each direction of the edge
## Using the adjacency matrix
dam<-as.matrix(get.adjacency(dixon))
sum(dam*t(dam)) ## Element-wise multiplication of the adjacency matrix, giving us the full number of mutual edges (equivalent to 2*dyad_census(dixon)$mut.)
sum(dam) ## The count of the total number of edges in the network (equivlaent to ecount(dixon).)
sum(dam*t(dam))/sum(dam)
reciprocity(dixon,mode="ratio")
## and we can get this same value from the dyad census...
dyad.census(dixon)$mut/(dyad.census(dixon)$mut + dyad.census(dixon)$asym) ## (count of mutual ties) / (count of mutual and asymmetric ties)
assortativity.nominal(dixon,V(dixon)$grade)
assortativity.nominal(dixon,V(dixon)$sex)
assortativity.nominal(dixon,factor(V(dixon)$race)) ## We have to specify that we want R to treat the "race" attribute as a factor, meaning with levels for each different "race" name.
transitivity(dixon)
rand<-sample_gnm(vcount(dixon),
ecount(dixon),
directed=TRUE) #vcount() gives us the number of vertices, ecount() gives us the number of edges
summary(rand)
transitivity(rand)
census_labels = c('003','012','102','021D','021U','021C','111D','111U','030T','030C','201','120D','120U','120C','210','300') ## this is the order in which the triads appear in the triad census output.
triad.census(dixon)
data.frame(census_labels,triad.census(dixon)) ## binding the two labels with the output will let us actually interpret this!
setwd("~/Social network analysis/Assigments/Week 5")
meta <- read.csv("meta_eurovision.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
euro_tele <- read.csv("euro_televoting_count_16-22.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
euro_jury <- read.csv("euro_jury_count_16-22.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
net_tele <- graph_from_data_frame(euro_tele, directed = TRUE)
net_jury <- graph_from_data_frame(euro_jury, directed = TRUE)
## creating a new dataframe to which we can append the metadata, appropriately aligned with the order of the nodes in the networks
df <- data.frame(Country = V(net_tele)$name)
df <- merge(df, meta, by = "Country", sort = FALSE)
## again creating the networks, this time adding all node attributes through the "vertices" element
net_tele <- graph_from_data_frame(euro_tele, vertices = df)
net_jury <- graph_from_data_frame(euro_jury, vertices = df)
setwd("~/Social network analysis/Assigments/Week 5")
setwd("~/Social network analysis/Assigments/Week 5")
meta <- read.csv("meta_eurovision.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
euro_tele <- read.csv("euro_televoting_count_16-22.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
euro_jury <- read.csv("euro_jury_count_16-22.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
net_tele <- graph_from_data_frame(euro_tele, directed = TRUE)
library(readr)
library(igraph)
library(igraphdata)
library(stargazer)
library(dplyr)
library(tidyr)
library(tidyverse)
library(scales)
meta <- read.csv("meta_eurovision.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
euro_tele <- read.csv("euro_televoting_count_16-22.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
euro_jury <- read.csv("euro_jury_count_16-22.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
net_tele <- graph_from_data_frame(euro_tele, directed = TRUE)
net_jury <- graph_from_data_frame(euro_jury, directed = TRUE)
## creating a new dataframe to which we can append the metadata, appropriately aligned with the order of the nodes in the networks
df <- data.frame(Country = V(net_tele)$name)
df <- merge(df, meta, by = "Country", sort = FALSE)
## again creating the networks, this time adding all node attributes through the "vertices" element
net_tele <- graph_from_data_frame(euro_tele, vertices = df)
net_jury <- graph_from_data_frame(euro_jury, vertices = df)
net_tele
net_tele
plot.igraph(net_tele,
vertex.size = strength(ecount(net_tele)),
edge.width = 0.3,
edge.arrow.size = 0.3,
vertex.label.cex = 0.6)
strength(ecount(net_tele))
meta <- read.csv("meta_eurovision.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
euro_tele <- read.csv("euro_televoting_count_16-22.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
euro_jury <- read.csv("euro_jury_count_16-22.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
net_graph <- graph_from_data_frame(euro_tele, directed = TRUE)
net_graph <- graph_from_data_frame(euro_jury, directed = TRUE)
meta <- read.csv("meta_eurovision.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
euro_tele <- read.csv("euro_televoting_count_16-22.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
euro_jury <- read.csv("euro_jury_count_16-22.csv", header = TRUE, as.is = TRUE, stringsAsFactors = FALSE)
net_tele_graph <- graph_from_data_frame(euro_tele, directed = TRUE)
net_jury_graph <- graph_from_data_frame(euro_jury, directed = TRUE)
## creating a new dataframe to which we can append the metadata, appropriately aligned with the order of the nodes in the networks
df <- data.frame(Country = V(net_tele_graph)$name)
df <- merge(df, meta, by = "Country", sort = FALSE)
## again creating the networks, this time adding all node attributes through the "vertices" element
net_tele <- graph_from_data_frame(euro_tele, vertices = df)
net_jury <- graph_from_data_frame(euro_jury, vertices = df)
strength(ecount(net_tele_graph))
ecount(net_tele_graph)
net_tele
net_tele
ecount(net_tele_graph)
plot.igraph(net_tele,
vertex.size = Count,
edge.width = 0.3,
edge.arrow.size = 0.3,
vertex.label.cex = 0.6)
net_tele
ecount(net_tele_graph)
plot.igraph(net_tele,
vertex.size = net_tele$Count,
edge.width = 0.3,
edge.arrow.size = 0.3,
vertex.label.cex = 0.6)
net_tele
ecount(net_tele_graph)
plot.igraph(net_tele,
vertex.size = scale(net_tele$Count, c(1,8)),
edge.width = 0.3,
edge.arrow.size = 0.3,
vertex.label.cex = 0.6)
net_tele
ecount(net_tele_graph)
plot.igraph(net_tele,
vertex.size = 0.5,
edge.width = net_tele$Count,
edge.arrow.size = 0.3,
vertex.label.cex = 0.6)
strength(net_tele$Count)
strength(net_tele)
?strength
strength(net_tele, weights = Count)
strength(net_tele, weights = net_tele$Count)
plot.igraph(net_tele,
vertex.size = strength(net_tele, weights = net_tele$Count),
edge.width = net_tele$Count,
edge.arrow.size = 0.3,
vertex.label.cex = 0.6)
setwd("C:/Users/crist/OneDrive - London School of Economics/London/Cross sectional market data/Rscripts/4. RMD Report Files")
setwd("G:/My Drive/Clases/LaboratoriodeDatos/Clase 3")
library("tidyverse")
library("htmlwidgets")
#install.packages("htmlwidgets")
current_dir <- "./"
sample_text <- read_file(paste(current_dir, "sample_texts.txt", sep = ""))
str_view(sample_text, "@")
str_view_all(sample_text, "@")
str_view(sample_text, "2")
library("tidyverse")
library("htmlwidgets")
#install.packages("htmlwidgets")
current_dir <- "./"
sample_text <- read_file(paste(current_dir, "sample_texts.txt", sep = ""))
str_view(sample_text, "@")
str_view_all(sample_text, "@")
str_view_all(sample_text, "a|e")
str_view_all(sample_text, "Münster")
str_extract(sample_text, "\\d{1,2}[:.\\s-]?\\d{0,2}")
str_extract(sample_text, "\d{1,2}[:.\s-]?\d{0,2}")
str_extract(sample_text, "\\d{1,2}[:.\\s-]?\\d{0,2}")
object=str_extract_all(sample_text, "\\d{1,2}[:.\\s-]?\\d{0,2}")
object
library("quanteda")
library("quanteda.textplots")
library("tidyverse")
library("scales")
exemplary_documents <- c("This is the text of a first document.", "This is a text of a second document.", "And of a third document.")
names(exemplary_documents) <- c("Document A", "Document B", "Document C")
exemplary_documents
exemplary_corpus <- corpus(exemplary_documents,docvars = data.frame(name = names(exemplary_documents),characters = str_count(exemplary_documents)))
exemplary_corpus
data_corpus_inaugural
head(docvars(data_corpus_inaugural))
dfmplain <- data_corpus_inaugural %>%
tokens() %>%
dfm()
dfmplain
dfmplain <- data_corpus_inaugural %>%
tokens(remove_punct = TRUE) %>%
dfm()
dfmplain
short_dictionary <- dictionary(list(taxation = c("tax", "taxes", "taxation"),unemployment = c("unemployment", "unemployed")))
short_dictionary
short_dictionary_glob <- dictionary(list(taxation = c("*tax*"),
unemployment = c("unemploy*")))
short_dictionary_glob
dfmplain %>% dfm_select(pattern = "*tax*", valuetype = "glob") %>% featnames()
short_dictionary_regex <- dictionary(list(taxation = c("tax[a-z]*"),
unemployment = c("unemploy[a-z]*"))
)
dfmplain %>% dfm_select(pattern = "tax[a-z]*", valuetype = "regex") %>% featnames()
dfm_dictionary <- dfm_lookup(dfmplain, dictionary = short_dictionary_glob,
valuetype = "glob") # note that the function is case insensitive by default
dfm_dictionary
dfm_dictionary_relative <- dfm_dictionary/rowSums(dfmplain)
dfm_dictionary_relative
# Transformar a data frame
df_plot <- convert(dfm_dictionary_relative, to = "data.frame")
# Graficar
p <- ggplot(df_plot, aes(x=taxation, y=unemployment, label=doc_id))
pq <- p + geom_point() + geom_text(hjust=-.1, size=3) +
theme_minimal() +
scale_x_continuous(expand = c(0, .001)) +
scale_y_continuous(labels = comma) +
xlab("Taxation") +
ylab("Unemployment")
pq
dfmcleaned <- data_corpus_inaugural %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_remove(stopwords("en")) %>%
dfm()
dfmcleaned
textplot_wordcloud(dfm_subset(dfmcleaned, President == "Obama"),
rotation=0, min_size=.75, max_size=3, max_words=50)
textplot_wordcloud(dfm_subset(dfmcleaned, President == "Trump"),
rotation=0, min_size=.75, max_size=3, max_words=50)
textplot_wordcloud(dfm_subset(dfmplain, President == "Obama"),
rotation=0, min_size=.75, max_size=3, max_words=50)
principia <- pdf_text("principia.pdf")
#install.packages("tesseract")
#install.packages("pdftools")
library("pdftools")
library("stringr")
library("quanteda")
principia <- pdf_text("principia.pdf")
class(principia)
length(principia)
principia
principia
principia_corpus <- principia %>% corpus(
docvars = data.frame(page=1:length(principia)))
principia_dfm <- principia_corpus %>%
tokens() %>%
dfm()
principia_dfm
general_theory <- pdf_ocr_text(pdf = "general_theory_cover.pdf", language = "eng", dpi = 300)
general_theory
